{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project\n",
    "\n",
    "## Predicting the Usage of Urban Bike Share Systems from Date and Meteorological Data\n",
    "\n",
    "<img src=\"./Bike-Sharing-Dataset/cover.png\" width=\"30%\" height=\"30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "- Selim Shaalan\n",
    "- Dou Hoon Kwark\n",
    "- Chris Myers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The objective of our project is to predict the usage rate of the Capital Bikeshare system in Washington D.C. Our analysis is based on an hourly dataset containing weather observations (such as temperature, humidity, and wind speed), the day of the week, time of year, and the number of bikes rented in each hour. By analyzing this data, we aimed to identify the factors that drive or inhibit bike rentals and thus developed a model that can accurately forecast the number of bike rentals for any given day or hour, provided we have information about the weather conditions and time of year. To achieve this, we started by cleaning the data. Next, we trained a variety of regression models on this dataset, including linear regression, KNN, Random Forest, as well as SVM, and then select the model that performs best after hyperparameter tuning. Our evaluation metrics are RMSLE and R2 of the testing set. The best model is chosen based on lower RMSLE(difference between prediction and target) and higher R2(how well the model captures the dataset). As a result, Random Forest and KNN perform well on the given dataset as they well represent the dataset and produce high performance in prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The bike sharing dataset by capital bikeshare is a very well maintained dataset, with reports being published on their main website on a monthly basis<a href=\"https://capitalbikeshare.com/system-data\"><sup>[1]</sup></a>. It provides a selection of different variables that can be used to predict the amount of bikes that get shared on a daily or hourly basis, which makes it ideal for machine learning. This issue is relevant because figuring out the demand for these bikes allows bikeshare companies to provide a supply accordingly, and other stakeholders such as ridesharing or public transport services adjust their models using this information as well. Consequently, there have been some efforts on Kaggle to create models for this dataset, although these projects usually fall short in detail, since many of them forgo hyperparameter search<a href=\"https://www.kaggle.com/competitions/bike-sharing-demand/code\"><sup>[2]</sup></a>. While there are many that do not, they are constrained by Kaggle’s challenge to only use this specific dataset, which does not allow us to ask questions such as “Are bike rentals affected by rain?” Without relying solely on intuition suggesting this hypothesis to be true, further studies in Cracow, Poland reveals that there is a significant link between bike sharing systems and the amount of rain on that day<a href=\"https://doi.org/10.1016/j.scs.2021.102724\"><sup>[3]</sup></a>. \n",
    "\n",
    "Furthermore, we wanted to examine if the day and weather conditions had similar impacts on other vehicle sharing services such as Uber. Research has been conducted on this phenomenon independently through a study of Uber rides in New York and the weather, but it remains to be determined whether a similar or inverse trend will be observed when compared to the number of bike rentals during the same period since bikes and cars are usually thought of as substitute goods<a href=\"https://support.sas.com/resources/papers/proceedings17/1260-2017.pdf\"><sup>[4]</sup></a>.\n",
    "\n",
    "Finally, a paper by Gama and Fanaee-T makes liberal use of this dataset and tests out different modeling techniques for it, although designing an optimized model and minimizing errors was not the primary purpose of the paper<a href=\"https://www.researchgate.net/publication/259009057_Event_labeling_combining_ensemble_detectors_and_background_knowledge\"><sup>[5]</sup></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The past decade has seen the rise of micromobility solutions, including electric scooters, traditional bicycles, and e-bikes, which offer a convenient mode of transportation for medium-length distances in cities worldwide. While these devices offer numerous benefits to users, their operation poses significant challenges to the companies that manage them. Maintaining an adequate supply of charged, operational vehicles to meet demand is critical to prevent lost profits and keep customers satisfied.\n",
    "\n",
    "To address this issue, we aim to use supervised machine learning to predict the demand for the Washington D.C. Capital Bikeshare system, which comprises over 5,400 bikes. Our goal is to accurately forecast the number of bikes that will be reserved each hour and day, providing the company with sufficient supply to meet demand. To achieve this, we will leverage existing data on weather, seasonality, and rental usage collected in 2011 at an hourly and daily granularity.\n",
    "\n",
    "Quantifying and measuring data extracted from weather and seasonal information have been shown to be reliable and widely used in many fields, such as weather forecasting. By quantifying and measuring our target problem and solution, we can expect reproducible predictions. Additionally, the dataset encompasses the entire two years, further increasing the replicability of our results. \n",
    "\n",
    "Therefore, we can use various regression models, such as linear regression, K-Nearest Neighbor, Support Vector Machine. For instance, linear regression can be used to model the linear relationship between features and target variable, such as weather and seasonal information, and predict the number of bicycle rentals for a given hour and day. Also, KNN and SVM models are good candidates to solve the problem because their ability to capture the variance of the dataset and handle the complexity.\n",
    "\n",
    "By applying the supervised machine learning techniques, we aim to build an accurate model that can forecast bike rental demand, allowing the company to optimize its supply chain and enhance its customer service.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "#### Dataset 1: Bike Sharing Dataset\n",
    "The dataset is available from UCI Machine Learning Repository at [here](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) or\n",
    "[Direct link to the dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/00275/)\n",
    "\n",
    "This is a dataset with information about the amount of bikes rented from Capital Bikeshare in the years 2011-2012. It has been combined with weather data in that same time period as well as day and holiday data. The dataset has 17389 rows and 16 variables including one column for the number of bikes rented. Every row corresponds to information about what hour/day it is, what the weather was like and how many bikes were rented during that period.\n",
    "\n",
    "In the dataset, an observation consists of the following:\n",
    "- instant : record index\n",
    "- dteday : The date in MM/DD/YYYY format\n",
    "- season : season (1:winter, 2:spring, 3:summer, 4:fall)\n",
    "- yr : year (0: 2011, 1:2012)\n",
    "- mnth : month ( 1 to 12)\n",
    "- hr : hour (0 to 23)\n",
    "- holiday : Whether the given day is a U.S holiday or not \n",
    "- weekday : The day of the week\n",
    "- workingday : if day is neither weekend nor holiday, the value is 1, otherwise it is 0.\n",
    "- weathersit :\n",
    "  - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "  - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "  - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "  - 4: Heavy Rain + Ice Pellets + Thunderstorm + Mist, Snow + Fog\n",
    "- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n",
    "- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "- casual: count of casual users\n",
    "- registered: count of registered users\n",
    "- cnt: count of total rental bikes including both casual and registered\n",
    "\n",
    "Also, it is worth noting that some features are dropped through feature engineering. The dropped features and reasoning are as follows:\n",
    "- instant : it does not produce anything meaningful as it was simply labelled to denote each observation.\n",
    "- dteday : because all the important factors have already been extracted and stored in separate features, it would not help produce better performance in predictions.\n",
    "- holiday : from the observation description, we can notice that feature holiday is already covered in workingday as the feature workingday accounts for both weekend and holiday.\n",
    "- temp : it has high correlation with atemp feature, which is almost close to 1. To avoid any potential issues with modeling (especially modeling the linear regression algorithms), it is necessary to drop either temp or atemp.\n",
    "- casual & registered : as our goal is to predict the number of total bicycles rented in the given hour, it is important to note that we need to set feature cnt as our target variable and drop feature casual and registered.\n",
    "\n",
    "Throughout feature engineering, we also separate categorical and numerical values so that we can increase the performance of models. Additionally, we also drop outliers that only hurt the performance of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We proposed building regression models based on weather and seasonal factors from the dataset to solve our stated problem. Our solution involves multivariate regression, K-NN, Random Forest, as well as SVM to model the relationship between the predictors and the response variable. Regression modeling is the most appropriate approach because our target prediction involves numerical values rather than classification. Our solution is applicable for several reasons:\n",
    "\n",
    "\n",
    "Multivariate regression: Multivariate regression is a popular and highly interpretable model for capturing the relationship between dependent and independent variables. Also, due to its high interpretability, we can generate a strong interpretable model if it fits well in our dataset. Therefore, it is a good starting point to model the relationship between the predictors and the response variables.\n",
    "\n",
    "\n",
    "K-NN(K-nearest neighbor): It is also known as a strong model that can be used as a regression  model to predict the target well if the dataset has a large number of observations and few features. The K-NN model tries to find the possible relationship between our desired prediction and k nearest data points that the model remembers. In our dataset, there are more than 17000 observations, while there are less than 20 features to consider to train our model. Therefore, K-NN produces a good performance.\n",
    "\n",
    "\n",
    "Random Forest: Random Forest is an ensemble learning algorithm that combines multiple decision trees so that it improves the accuracy in prediction. Also, it can handle large datasets even with high dimensionality while it is less prone to overfitting in dataset. Additionally, it provides insights into each feature and importance, which allows us to identify which variables have the most significant impact on the target variable.\n",
    "\n",
    "\n",
    "SVM(Support Vector Machine): SVM is a flexible and robust model that provides high performance in regression prediction. Also, it draws strong hyperplans based on the observations in the dataset to capture possible complex variable relationships. Therefore, it fits well when it comes to all the different types of potential underlying variable relationships that we want to capture.\n",
    "\n",
    "\n",
    "To evaluate the performance of the suggested models, we use k-fold cross-validation to estimate their accuracy on new data, and we use grid search to optimize the hyperparameters of each model. We implement the models using the sklearn Python library to ensure reproducibility and minimize implementation errors.\n",
    "\n",
    "\n",
    "In summary, we are confident that our proposed solution is the best fit for predicting bicycle rentals based on weather and seasonal factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We used both $R^2$ and $RMSLE$ as evaluation metrics to assess the performance of our benchmark model and solution model. Our goal is to create a robust regression model that can predict precise quantities of rental bicycles based on weather and seasonal factors.\n",
    "\n",
    "In building our regression model, we understand the importance of using both $R^2$ and $RMSLE$ metrics as they complement each other in providing an accurate picture of model performance. $R^2$ measures how well our data fits our regression model, while $RMSLE$ quantifies the error between the predicted and actual values of the model.\n",
    "\n",
    "The formulae for both metrics are as follows: \n",
    "\n",
    "$R^2$ = $1 - \\frac{RSS}{TSS}$, where $RSS$ represents the sum of squares of residuals and $TSS$ represents the total sum of squares. \n",
    "\n",
    "$RMSLE$ = $\\sqrt{\\frac{\\sum_{i=1}^{N}(log(y_{pred}+1) - log(y_{actual}+1))^2}{N}}$, where $y_{pred}$ represents the predicted value from our model, $y_{actual}$ represents the actual value, and $N$ represents the number of data points.\n",
    "\n",
    "By utilizing both $R^2$ and $RMSLE$, we can better evaluate the quality of our regression model and ensure that it accurately predicts the desired quantities of rental bicycles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Suitability (EDA) \n",
    "The details can be also found [here](Bike-EDA.ipynb)\n",
    "First of all, the total data size, (17379, 17) shows that we have enough size of observations to further our project.\n",
    "\n",
    "<img src=\"./Bike-Sharing-Dataset/crop1.png\" width=\"30%\"/>\n",
    "\n",
    "After initial drop of unnecessary columns, we can see that each feature consists of intuitive and clean numbers.\n",
    "\n",
    "<img src=\"./Bike-Sharing-Dataset/crop2.png\" width=\"40%\"/>\n",
    "\n",
    "#### Baxplot\n",
    "Then, we can see that relationship between bicycle counts and each feature.\n",
    "\n",
    "<img src=\"./Bike-Sharing-Dataset/output1.png\" width=\"70%\"/>\n",
    "\n",
    "The plots suggest meaningful observations:\n",
    "1. As the season changes, the rental bicycle count also changes.\n",
    "2. Then rental bicycle count is also related with month, hour of the day, weather condition.\n",
    "3. There is a relatively small difference between rental count on working day and non-working day, but we can see that on data points  from working day there are more existing outliers.\n",
    "\n",
    "#### Barplot & Pointplot\n",
    "<img src=\"./Bike-Sharing-Dataset/output2.png\" width=\"50%\"/>\n",
    "\n",
    "Here, we can also observe that depending on month, season, weekdays, there are noticeable differences in average rental count.\n",
    "The plots also suggest the following:\n",
    "1. During seasons when bicycling conditions are more favorable, we see an increase in bicycle rentals.\n",
    "2. The average rental counts vary depending on the time of day. During weekdays, people tend to commute between 7-8 am and 5-6 pm, while during weekends and holidays, they tend to enjoy their leisure time in the afternoon. Therefore, we observe higher rental counts during these periods.\n",
    "\n",
    "#### HeatMap/Correlation\n",
    "<img src=\"./Bike-Sharing-Dataset/heat_map_new.png\" width=\"50%\"/>\n",
    "\n",
    "It is noticeable that atemp and temp have high correlation that is close to 1, which shows that we need to drop one of them to avoid multi-collinearity.\n",
    "\n",
    "#### EDA Conclusion\n",
    "\n",
    "1. The rental count of bicycles changes with the season, month, hour of the day, and weather condition. The dataset shows a significant increase in rental count during favorable bicycling conditions, such as during the summer and fall seasons.\n",
    "\n",
    "2. The average rental counts also vary depending on the time of day. During weekdays, people tend to commute between 7-8 am and 5-6 pm, while during weekends and holidays, they tend to enjoy their leisure time in the afternoon. Therefore, we observe higher rental counts during these periods.\n",
    "\n",
    "3. There is a relatively small difference between rental count on working day and non-working day, but we can see that on data points from working day there are more existing outliers. This suggests that the presence of outliers should be considered when building the regression model.\n",
    "\n",
    "4. The correlation heatmap shows that there are strong positive correlations between the target variable (rental count) and some of the features, such as temperature and humidity.\n",
    "\n",
    "5. The count over continuous variables plot shows the distribution of the target variable for different values of the continuous variables. This plot indicates that temperature and humidity have a significant impact on the rental count of bicycles.\n",
    "\n",
    "6. Based on the EDA, we can conclude that the problem can be solved using regression models. We can build a model that predicts the rental count of bicycles based on the values of the features such as season, month, hour, weather condition, temperature, and humidity.\n",
    "\n",
    "Overall, the EDA provides insights into the relationships between the features and the target variable, and helps us to identify the key factors that influence the rental count of bicycles. This information can be used to develop a predictive model that accurately predicts the rental count of bicycles, and can help businesses optimize their operations and resources accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "The details can be also found [here](Bike-EDA.ipynb)\n",
    "\n",
    "<img src=\"./Bike-Sharing-Dataset/crop3.png\" width=\"50%\"/>\n",
    "\n",
    "After feature engineering, selection, these are the final features that we build our models on. Also, it is worth noting that instead of manual one-hot encoding, we have categorized feature season, mnth, hr, weekday, workingday, weathersit. Therefore, these features are not treated as continous variables during modeling.\n",
    "\n",
    "Also, after dividing the dataset into training set and testing set, we transformed our target value with function log(1 + x) to avoid having negative values and huge difference between max and min values. \n",
    "\n",
    "<img src=\"./Bike-Sharing-Dataset/output4.png\" width=\"30%\"/> to\n",
    "<img src=\"./Bike-Sharing-Dataset/output5.png\" width=\"50%\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of base model (Linear Regression Model)\n",
    "The details can be also found [here](Bike-EDA.ipynb)\n",
    "<br>\n",
    "<img src=\"./Bike-Sharing-Dataset/crop4.png\" width=\"50%\"/>\n",
    "\n",
    "From our default algorithm, Linear Regression, we have obtained 1.041 for its RMSLE value and 0.1174 for its R2 score. We can explain only 11.7% of the variance in the data. It means that our base model has poor performance to capture the given dataset since RMSLE value is high and R2 score is close to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Error & Cross validation Error Curve\n",
    "The details can be also found [here](Bike-EDA.ipynb)\n",
    "<br>\n",
    "<img src=\"./Bike-Sharing-Dataset/output6.png\" width=\"35%\"/>\n",
    "\n",
    "<img src=\"./Bike-Sharing-Dataset/output7.png\" width=\"35%\"/>\n",
    "\n",
    "These are the training error/CV error curves based on RMSLE and MSE performance, respectively. Except for the green curve from the second plot, every other curve rather shows fluctuation in its error. Therefore, from the plots, we can understand that our linear regression does not capture the dataset at all no matter the size of training examples. Also, it is worth mentioning that from the first plot, the plot suggests that the model possibly learns some characteristics of the dataset to some extent as it shows that overall trend of errors goes down as the sample size increases. Therefore, we further searching for the better models with changes to hyper parameters.\n",
    "\n",
    "It indicates that to improve the model's performance on the given dataset, it is crucial to explore other models that can deliver better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of the Base Model with Regularizers\n",
    "\n",
    "Afterwards, we proceeded to investigate ridge and lasso models, which are variants of regularized linear regression, using various alpha values.\n",
    "\n",
    "The following is the plot of curves for the performance based on different values for alpha.\n",
    "\n",
    "<img src=\"./Bike-Sharing-Dataset/output8.png\" width=\"35%\"/>\n",
    "\n",
    "Even though Lasso regression appears to be slightly better than Ridge regression, we can see that they both perform poorly on the dataset as their scores at most around 0.10. \n",
    "\n",
    "It introduces an idea that we need more complex model to capture the variance of the dataset, rather than linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Alternative Model #0 (KNN)\n",
    "The details can also be found [here](Bike-EDA.ipynb)\n",
    "<br>\n",
    "The following is the basic KNN model, which can be a powerful model depending on the complexity of dimension from the given dataset.\n",
    "\n",
    "<img src=\"./Bike-Sharing-Dataset/output10.png\" width=\"60%\"/>\n",
    "\n",
    "Here, it shows that KNN model has been able to capture approximately 80% of the variance of the dataset as well as brings down the RMSLE to ~0.19.\n",
    "\n",
    "Therefore, KNN model has shown better performance to capture the information of the given dataset since it does not limit itself to linear regression. Also, we can notice that there might be more potentially better models as long as the models well understand the complexity of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Alternative Model #1 (RandomForestRegressor)\n",
    "The details can also be found [here](Group19-RandomForest.ipynb)\n",
    "<br>\n",
    "<img src=\"./Bike-Sharing-Dataset/randomforest_default_scores.png\" width=\"50%\" height=\"50%\" />\n",
    "\n",
    "Our alternative algorithm, Random Forest Regression, performs far better than our Linear Regressional model, did with a peak R2 score of 0.862 and RMSLE of 0.153. Not too bad! We can interpret this as our random forest model explains 86.2% of the variance in the data. Also, it appears that it is the best model as our RMSLE is the lowest value so far. This indicates that our random forest regressor has high performance to capture the given dataset given a high R2 score and low RMSLE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Alternative Model #2 (Support Vector Machine)\n",
    "The details can also be found [here](Group019-SVM.ipynb)\n",
    "<br>\n",
    "\n",
    "The following is an SVR model that was grid searched with various kernels and C values. \n",
    "\n",
    "<img src=\"./Bike-Sharing-Dataset/svm1.PNG\" width=\"60%\"/>\n",
    "\n",
    "As we can see, even with our best grid search output, we only achieved a peak R2 score of 0.1584, and a peak RMSLE of 0.7676. After taking into account the RandomForest's performance, we can conclude that SVM is not a very good modeler of our dataset. It should be noted that with lower values of C, we achieved various negative r2'd and very high RMSLE values. This means that our model is performing worse than a linear line drawn through the dataset, and we would be better suited modeling the data using a random forest instead.\n",
    "\n",
    "Side note: We used a train-test split for this model as the cross_val took too long to run with our choices for the grid search\n",
    "\n",
    "\n",
    "<img src=\"./Bike-Sharing-Dataset/svm_2.PNG\" width=\"40%\"/> \n",
    "<img src=\"./Bike-Sharing-Dataset/svm_final_2.PNG\" width=\"40%\"/> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "- Random forest:\n",
    "  \n",
    "  The random forest classifier performed exceptionally well on the data compared to other models we test, capturing over 84% of the data’s variance as shown by the R2 score. Random forest regressors work by building many decision trees from subsets of the data and averaging their predictions, making them robust to noisy features in the data and overfitting. They are not as ideal when there are a small number of features or there is a strict linear relationship between the features and target variable, but this was not the case with our data as we had an extensive body of available features for training and there is no clear linear relationship between our predictors and predictions. Of note is that, based on our heat map, temperature had a significant positive correlation with the number of bikes rented and humidity had a significant negative correlation with bikes rented–though we cannot directly peek inside our forest to see how it makes decisions, we can be sure that these features have a large influence. Ultimately, this is the model we would use to predict future bike rentals as it generalizes best to unseen data.\n",
    "\n",
    "- Linear Regression / with regularizers:\n",
    "\n",
    "  It turned out that linear regression performs poorly on the given dataset. It is because after splitting values into categorical and numerical variables, our linear regression models take only numerical variables to fit themselves to the given training dataset. Also, from the heatmap, we can also see that not every numerical variables have strong correlation with count variable. Therefore, even with regularizers, linear regression models, including Lasso and Ridge do not work well on the dataset, which suggests that we need to use more complex models to understand and capture the given dataset.\n",
    "\n",
    "- K-NN:\n",
    "\n",
    "  K-NN appears that it is the second best model when it comes to comparing RMSLE and R2 with other models as it shows ~0.19 for RMSLE and ~0.80 for R2. Since the model capacity of K-NN is relatively strong enough to capture the complexity of dataset, it establishes good numbers that can be used to indicate the model capacities of other models. However, as we understand that there are possibly high noise and lack of useful features in the given dataset, K-NN does not overcome the factors to predict better values because it simply compares with the data points in training set, not reflecting any learning from the training set.\n",
    "\n",
    "- SVM:\n",
    " \n",
    "  The SVM model did not perform very well at all for predicting power. It had one of the lowest R2 values and one of the highest RMSLE values. This is to be expected since our dataset was very large, and we have a lot of noise since only a few of our variables have any predictive power, both conditions that significantly impact the performance of a SVM model. This model took an absurdly long time to grid search, and the cross validation attempts didn’t finish running after hours, so we opted for a train test split instead, and still had the grid search go upwards of an hour. \n",
    "\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "Throughout the project, we have noticed that hyper parameter tuning would take plenty of time since we have limited computational resources. We have tested several hyper parameters for each model, but as the number went up, the time machine takes to learn exponentially increased, which lead to testing the models only on the small sets of hyper parameters. Also, after dividing features into categorical and numerical features, there are relatively few features left to be trained by linear regression model. Because of greatly poor performance of linear regression model, even with regularizers, it was challenging to establish a concrete base performance that we can use to compare with other models. Since we were not able to access any additional features that can be combined and lead to better performance in prediction, it has also brought limitations of fully utilizing the dataset trained by the models we have implemented.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Our dataset itself does not have any part that could potentially raise issues concerning ethics and privacy since it only contains weather observations, seasonal information, and the hourly count of rental bicycle. Also, it has already been filtered by one of the reliable sources, UCI Machine Learning Repository. Nonetheless, in order to prevent any type of unpredictable issues that we would possibly overlook through further project process and collecting additional possible datasets, we observed the following items:\n",
    "\n",
    "- Data Privacy: We make sure that there is no issue regarding private information and the any form of information is always anonymized while we further our research and project.\n",
    "\n",
    "- Discrimination and Fairness: We make sure that there is no outcome from our model that potentially brings up any form of discrimination and fairness issues against any certain groups or individuals by always carefully choosing right options to develop our model.\n",
    "\n",
    "- Transparency and Explainability: We make sure that the process of our research and project is transparent as well as explainable so that our project can be a reliable and trustworthy source to machine learning community. \n",
    "\n",
    "- Intellectual property: we make sure that there is no intellectual property infringement by properly handling ideas from any other resources and observing academic integrity if any. Also, if we have any concerns about the regarding issues, we make sure that we always discuss with our course instructing team.\n",
    "  \n",
    "In addition to the items listed above, we make sure that we always utilize the tools that helps our project healthy in terms of ethics and privacy, such as checklist from https://deon.drivendata.org.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Overall, we conclude that the random forest model was the one with strongest predictive power because its strengths matched the features of our dataset. It consistently performed better than its counterparts in almost all cross validation attempts which demonstrates the power of ensemble learning for tasks like these. We found that with our finely tuned random forest model with the best parameters, we are able to achieve an R2 value of 0.862 and a RMSLE of 0.153."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "<a id=\"System\"></a>1.[^](#System): System data: Capital bikeshare. Capital Bikeshare: Metro DC's bikeshare service. (n.d.). Retrieved February 22, 2023, from https://capitalbikeshare.com/system-data <br> \n",
    "<a id=\"Bike\"></a>2.[^](#Bike): Bike sharing demand. Kaggle. (n.d.). Retrieved February 22, 2023, from https://www.kaggle.com/competitions/bike-sharing-demand/code <br>\n",
    "<a id=\"Pazdan\"></a>3.[^](#Pazdan): Pazdan, S., Kiec, M., &amp; D’Agostino, C. (2021). Impact of environment on Bicycle Travel Demand—assessment using bikeshare System data. Sustainable Cities and Society, 67, 102724. https://doi.org/10.1016/j.scs.2021.102724 <br>\n",
    "<a id=\"Brodeur\"></a>4.[^](#Brodeur): Brodeur, Abel & Nield, Kerry. (2018). An empirical analysis of taxi, Lyft and Uber rides: Evidence from weather shocks in NYC. Journal of Economic Behavior & Organization. 152. 1-16. 10.1016/j.jebo.2018.06.004. https://support.sas.com/resources/papers/proceedings17/1260-2017.pdf <br>\n",
    "<a id=\"Fanaee\"></a>5.[^](#Fanaee): Fanaee-T, Hadi & Gama, João. (2014). Event labeling combining ensemble detectors and background knowledge. Progress in Artificial Intelligence. 2. 113-127. 10.1007/s13748-013-0040-3. https://www.researchgate.net/publication/259009057_Event_labeling_combining_ensemble_detectors_and_background_knowledge \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
